<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How to Get LLM Keys</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      padding: 20px;
      background-color: #f4f5f7;
      color: #172b4d;
    }
    h1 {
      text-align: center;
      color: #091e42;
    }
    ul {
      list-style: none;
      padding: 0;
    }
    li {
      margin-bottom: 10px;
    }
    a {
      color: #0052cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .section {
      margin-top: 30px;
    }
  </style>
</head>
<body>
  <h1>How to Get LLM API Keys</h1>
  <p>Click on the provider to go to their API key generation page.</p>
  <ul>
    <li><a href="https://aistudio.google.com/app/apikey" target="_blank">Google Gemini</a></li>
    <li><a href="https://platform.openai.com/api-keys" target="_blank">OpenAI</a></li>
    <li><a href="https://console.anthropic.com/settings/keys" target="_blank">Anthropic</a></li>
    <li><a href="https://console.groq.com/keys" target="_blank">Groq</a></li>
    <li><a href="https://platform.deepseek.com/api_keys" target="_blank">Deepseek</a></li>
    <li><a href="https://openrouter.ai/keys" target="_blank">OpenRouter</a></li>
  </ul>

  <div class="section">
    <h2>Local Models</h2>
    <p>
      For Ollama: Download and install from <a href="https://ollama.com" target="_blank">ollama.com</a>.
      Run <code>ollama serve</code>. Use base URL: http://localhost:11434
    </p>
    <p>
      For LM Studio: Download from <a href="https://lmstudio.ai" target="_blank">lmstudio.ai</a>.
      Enable local inference server in settings. Default base URL: http://localhost:1234
    </p>
  </div>
</body>
</html>